{
    "task_id": "HumanEval/49",
    "prompt": "\n\ndef modp(n: int, p: int):\n    \"\"\"Return 2^n modulo p (be aware of numerics).\n    >>> modp(3, 5)\n    3\n    >>> modp(1101, 101)\n    2\n    >>> modp(0, 101)\n    1\n    >>> modp(3, 11)\n    8\n    >>> modp(100, 101)\n    1\n    \"\"\"\n",
    "canonical_solution": "    ret = 1\n    for i in range(n):\n        ret = (2 * ret) % p\n    return ret\n",
    "test": "\n\nMETADATA = {}\n\n\ndef check(candidate):\n    assert candidate(3, 5) == 3\n    assert candidate(1101, 101) == 2\n    assert candidate(0, 101) == 1\n    assert candidate(3, 11) == 8\n    assert candidate(100, 101) == 1\n    assert candidate(30, 5) == 4\n    assert candidate(31, 5) == 3\n\n",
    "entry_point": "modp",
    "coverage": 100.0,
    "num_iterations_used": 2,
    "msg_trace": {
        "1": {
            "generated_test_script": "from codebase import *\nimport unittest\nfrom codebase import modp\n\nclass TestModp(unittest.TestCase):\n\n    def test_modp(self):\n        test_cases = [\n            (3, 5, 3),\n            (1101, 101, 2),\n            (0, 101, 1),\n            (3, 11, 8),\n            (100, 101, 1)\n        ]\n\n        for n, p, expected in test_cases:\n            with self.subTest(n=n, p=p, expected=expected):\n                self.assertEqual(modp(n, p), expected)\n\nif __name__ == '__main__':\n    unittest.main()",
            "executed_output": "Name                                                                                           Stmts   Miss  Cover   Missing\n----------------------------------------------------------------------------------------------------------------------------\n/home/pc/Documents/Coverage-Test-Agent/test_coverage_multiagents/temp_test_gpt35/codebase.py       5      0   100%\n----------------------------------------------------------------------------------------------------------------------------\nTOTAL                                                                                              5      0   100%\n",
            "coverage": 100.0,
            "exist_error": false
        },
        "2": {
            "generated_test_script": "from codebase import *\nimport unittest\nfrom codebase import modp\n\nclass TestModp(unittest.TestCase):\n    \n    def test_modp_1(self):\n        self.assertEqual(modp(3, 5), 3)\n    \n    def test_modp_2(self):\n        self.assertEqual(modp(1101, 101), 2)\n    \n    def test_modp_3(self):\n        self.assertEqual(modp(0, 101), 1)\n    \n    def test_modp_4(self):\n        self.assertEqual(modp(3, 11), 8)\n    \n    def test_modp_5(self):\n        self.assertEqual(modp(100, 101), 1)\n\nif __name__ == '__main__':\n    unittest.main()",
            "executed_output": "Name                                                                                           Stmts   Miss  Cover   Missing\n----------------------------------------------------------------------------------------------------------------------------\n/home/pc/Documents/Coverage-Test-Agent/test_coverage_multiagents/temp_test_gpt35/codebase.py       5      0   100%\n----------------------------------------------------------------------------------------------------------------------------\nTOTAL                                                                                              5      0   100%\n",
            "coverage": 100.0,
            "exist_error": false
        }
    },
    "best_score_generated_test_script": "from codebase import *\nimport unittest\nfrom codebase import modp\n\nclass TestModp(unittest.TestCase):\n    \n    def test_modp_1(self):\n        self.assertEqual(modp(3, 5), 3)\n    \n    def test_modp_2(self):\n        self.assertEqual(modp(1101, 101), 2)\n    \n    def test_modp_3(self):\n        self.assertEqual(modp(0, 101), 1)\n    \n    def test_modp_4(self):\n        self.assertEqual(modp(3, 11), 8)\n    \n    def test_modp_5(self):\n        self.assertEqual(modp(100, 101), 1)\n\nif __name__ == '__main__':\n    unittest.main()",
    "first_generated_test_script": "from codebase import *\nimport unittest\nfrom codebase import modp\n\nclass TestModp(unittest.TestCase):\n\n    def test_modp(self):\n        test_cases = [\n            (3, 5, 3),\n            (1101, 101, 2),\n            (0, 101, 1),\n            (3, 11, 8),\n            (100, 101, 1)\n        ]\n\n        for n, p, expected in test_cases:\n            with self.subTest(n=n, p=p, expected=expected):\n                self.assertEqual(modp(n, p), expected)\n\nif __name__ == '__main__':\n    unittest.main()",
    "not_error_best_generated_test_script": "from codebase import *\nimport unittest\nfrom codebase import modp\n\nclass TestModp(unittest.TestCase):\n    \n    def test_modp_1(self):\n        self.assertEqual(modp(3, 5), 3)\n    \n    def test_modp_2(self):\n        self.assertEqual(modp(1101, 101), 2)\n    \n    def test_modp_3(self):\n        self.assertEqual(modp(0, 101), 1)\n    \n    def test_modp_4(self):\n        self.assertEqual(modp(3, 11), 8)\n    \n    def test_modp_5(self):\n        self.assertEqual(modp(100, 101), 1)\n\nif __name__ == '__main__':\n    unittest.main()",
    "filtered_generated_test_script": "from codebase import *\nimport unittest\nfrom codebase import modp\n\nclass TestModp(unittest.TestCase):\n    \n    def test_modp_1(self):\n        self.assertEqual(modp(3, 5), 3)\n    \n    def test_modp_2(self):\n        self.assertEqual(modp(1101, 101), 2)\n    \n    def test_modp_3(self):\n        self.assertEqual(modp(0, 101), 1)\n    \n    def test_modp_4(self):\n        self.assertEqual(modp(3, 11), 8)\n    \n    def test_modp_5(self):\n        self.assertEqual(modp(100, 101), 1)\n\nif __name__ == '__main__':\n    unittest.main()",
    "exist_error": false,
    "best_score": 100.0,
    "first_score": 100.0,
    "not_error_best_score": 100.0,
    "exist_not_error": true,
    "filtered_score": 100.0,
    "use_filter": false
}