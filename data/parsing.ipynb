{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import re, ast, astunparse\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 6.52k/6.52k [00:00<00:00, 51.0MB/s]\n",
      "Downloading data: 100%|██████████| 83.9k/83.9k [00:00<00:00, 88.9kB/s]\n",
      "Generating test split: 100%|██████████| 164/164 [00:00<00:00, 19413.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanEval dataset saved in HumanEval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "save_path = \"HumanEval\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for split in dataset.keys():\n",
    "    for i in range(len(dataset[split])):\n",
    "        json.dump(dataset[split][i], open(f\"{save_path}/{i}.json\", \"w\"), indent=4)\n",
    "\n",
    "print(f\"HumanEval dataset saved in {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HumanEvalCoverageParsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_parsing_path = \"HumanEvalCoverage\"\n",
    "os.makedirs(save_parsing_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py       8      1    88%   20\n",
      "test.py         9      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          17      1    94%\n",
      "\n",
      "Coverage: 88.0\n",
      "59\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      13      1    92%   12\n",
      "test.py        10      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          23      1    96%\n",
      "\n",
      "Coverage: 92.0\n",
      "81\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      29      3    90%   34, 42, 44\n",
      "test.py        11      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          40      3    92%\n",
      "\n",
      "Coverage: 90.0\n",
      "89\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py       8      1    88%   19\n",
      "test.py        12      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          20      1    95%\n",
      "\n",
      "Coverage: 88.0\n",
      "99\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      14      2    86%   25, 36\n",
      "test.py         9      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          23      2    91%\n",
      "\n",
      "Coverage: 86.0\n",
      "124\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      16      1    94%   33\n",
      "test.py        20      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          36      1    97%\n",
      "\n",
      "Coverage: 94.0\n",
      "127\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      16      4    75%   27-30\n",
      "test.py        12      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          28      4    86%\n",
      "\n",
      "Coverage: 75.0\n",
      "140\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      19      1    95%   29\n",
      "test.py         9      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          28      1    96%\n",
      "\n",
      "Coverage: 95.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset['test'])): \n",
    "    with open(f'HumanEval/{i}.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        method = data['prompt'] + data['canonical_solution']\n",
    "        method_name = data['entry_point']\n",
    "        with open('method.py', 'w') as out:\n",
    "            out.write(method)\n",
    "\n",
    "        test = \"\"\n",
    "        tree = ast.parse(method)\n",
    "        functions = [node.name for node in tree.body if isinstance(node, ast.FunctionDef)]\n",
    "        for j in range(len(functions)):\n",
    "            test += f\"from method import {functions[j]}\\n\"\n",
    "        test += \"\\n\\n\"\n",
    "        test += data['test']\n",
    "        test += f\"\\nif __name__ == '__main__':\\n    check({method_name})\"\n",
    "        with open('test.py', 'w') as out:\n",
    "            out.write(test)\n",
    "\n",
    "    subprocess.run([\"coverage\", \"run\", \"test.py\"], check=True)\n",
    "    result = subprocess.run([\"coverage\", \"report\", \"-m\"], capture_output=True, text=True, check=True)\n",
    "    match = re.search(r'method.py.* (\\d+%)', result.stdout)\n",
    "    if match:\n",
    "        coverage_percentage = match.group(1)\n",
    "        coverage_percentage = float(coverage_percentage[:-1])\n",
    "        if coverage_percentage != 100:\n",
    "            print(i)\n",
    "            print(result.stdout)\n",
    "            print(f\"Coverage: {coverage_percentage}\")\n",
    "    else:\n",
    "        coverage_percentage = None\n",
    "        print(\"Could not find coverage percentage.\")\n",
    "    data['coverage'] = coverage_percentage\n",
    "    with open(f'{save_parsing_path}/{i}.json', 'w') as out:\n",
    "        json.dump(data, out, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HumanEvalCoverageTestBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_parsing_testbase_path = \"HumanEvalCoverageTestBase\"\n",
    "os.makedirs(save_parsing_testbase_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomTest(ast.NodeTransformer):\n",
    "    def visit_FunctionDef(self, node):\n",
    "        if node.name == \"check\":\n",
    "            node.body = random.choice(node.body)\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstTest(ast.NodeTransformer):\n",
    "    def visit_FunctionDef(self, node):\n",
    "        if node.name == \"check\":\n",
    "            node.body = node.body[:2]\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astor\n",
    "def generate_split_test_files(generated_test_path, filtered_dir, method_name, method):\n",
    "    code = open(generated_test_path).read()\n",
    "    tree = ast.parse(code)\n",
    "\n",
    "    # Extract the function definition and its body\n",
    "    func_def = next(node for node in tree.body if isinstance(node, ast.FunctionDef))\n",
    "\n",
    "    # Get the assertions\n",
    "    asserts = [stmt for stmt in func_def.body if isinstance(stmt, ast.Assert)]\n",
    "\n",
    "    # Split into separate files\n",
    "    for i, assertion in enumerate(asserts, 1):\n",
    "        new_tree = ast.Module(body=[\n",
    "            ast.FunctionDef(\n",
    "                name=\"check\",\n",
    "                args=func_def.args,\n",
    "                body=[assertion],\n",
    "                decorator_list=[]\n",
    "            ),\n",
    "            ast.parse(f\"if __name__ == '__main__':\\n    check({method_name})\")\n",
    "        ], type_ignores=[])\n",
    "\n",
    "        # Convert the AST back to source code\n",
    "        new_code = astor.to_source(new_tree)\n",
    "        tree = ast.parse(method)\n",
    "        functions = [node.name for node in tree.body if isinstance(node, ast.FunctionDef)]\n",
    "        for j in range(len(functions)):\n",
    "            new_code = f\"from method import {functions[j]}\\n\" + new_code\n",
    "\n",
    "        # Write to a new file\n",
    "        filename = f\"{filtered_dir}/assertion_{i}.py\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(new_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset['test'])): \n",
    "    with open(f'HumanEval/{i}.json') as f:\n",
    "        data = json.load(f)\n",
    "        test = data['test']\n",
    "        \n",
    "        if i in [32, 38, 50]:\n",
    "            data['testbase'] = test\n",
    "        elif i in [44, 53, 151]:\n",
    "            tree = ast.parse(test)\n",
    "            transformer = FirstTest()\n",
    "            new_tree = transformer.visit(tree)\n",
    "            new_test = astunparse.unparse(new_tree)\n",
    "            data['testbase'] = new_test\n",
    "            \n",
    "        else:\n",
    "            new_test = ''\n",
    "            tree = ast.parse(test)\n",
    "            transformer = RandomTest()  \n",
    "            new_tree = transformer.visit(tree)\n",
    "            new_test = astunparse.unparse(new_tree)\n",
    "            data['testbase'] = new_test\n",
    "        with open(f'{save_parsing_testbase_path}/{i}.json', 'w') as out:\n",
    "            json.dump(data, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py       8      1    88%   20\n",
      "test.py         9      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          17      1    94%\n",
      "\n",
      "Coverage: 88.0\n",
      "59\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      13      1    92%   12\n",
      "test.py        10      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          23      1    96%\n",
      "\n",
      "Coverage: 92.0\n",
      "81\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      29      3    90%   34, 42, 44\n",
      "test.py        11      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          40      3    92%\n",
      "\n",
      "Coverage: 90.0\n",
      "89\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py       8      1    88%   19\n",
      "test.py        12      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          20      1    95%\n",
      "\n",
      "Coverage: 88.0\n",
      "99\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      14      2    86%   25, 36\n",
      "test.py         9      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          23      2    91%\n",
      "\n",
      "Coverage: 86.0\n",
      "124\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      16      1    94%   33\n",
      "test.py        20      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          36      1    97%\n",
      "\n",
      "Coverage: 94.0\n",
      "127\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      16      4    75%   27-30\n",
      "test.py        12      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          28      4    86%\n",
      "\n",
      "Coverage: 75.0\n",
      "140\n",
      "Name        Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------\n",
      "method.py      19      1    95%   29\n",
      "test.py         9      0   100%\n",
      "-----------------------------------------\n",
      "TOTAL          28      1    96%\n",
      "\n",
      "Coverage: 95.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset['test'])): \n",
    "    with open(f'{save_parsing_testbase_path}/{i}.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        method = data['prompt'] + data['canonical_solution']\n",
    "        method_name = data['entry_point']\n",
    "        with open('method.py', 'w') as out:\n",
    "            out.write(method)\n",
    "\n",
    "        test = \"\"\n",
    "        tree = ast.parse(method)\n",
    "        functions = [node.name for node in tree.body if isinstance(node, ast.FunctionDef)]\n",
    "        for j in range(len(functions)):\n",
    "            test += f\"from method import {functions[j]}\\n\"\n",
    "        test += \"\\n\\n\"\n",
    "        test += data['test']\n",
    "        test += f\"\\nif __name__ == '__main__':\\n    check({method_name})\"\n",
    "        with open('test.py', 'w') as out:\n",
    "            out.write(test)\n",
    "\n",
    "    subprocess.run([\"coverage\", \"run\", \"test.py\"], check=True)\n",
    "    result = subprocess.run([\"coverage\", \"report\", \"-m\"], capture_output=True, text=True, check=True)\n",
    "    match = re.search(r'method.py.* (\\d+%)', result.stdout)\n",
    "    if match:\n",
    "        coverage_percentage = match.group(1)\n",
    "        coverage_percentage = float(coverage_percentage[:-1])\n",
    "        if coverage_percentage != 100:\n",
    "            print(i)\n",
    "            print(result.stdout)\n",
    "            print(f\"Coverage: {coverage_percentage}\")\n",
    "    else:\n",
    "        coverage_percentage = None\n",
    "        print(\"Could not find coverage percentage.\")\n",
    "    data['coverage'] = coverage_percentage\n",
    "    with open(f'{save_parsing_testbase_path}/{i}.json', 'w') as out:\n",
    "        json.dump(data, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for i in range(len(dataset['test'])):\n",
    "    if i not in [32, 38, 50, 44, 53, 151]:\n",
    "        with open(f'{save_parsing_testbase_path}/{i}.json') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            method = data['prompt'] + data['canonical_solution']\n",
    "            method_name = data['entry_point']\n",
    "            with open('method.py', 'w') as out:\n",
    "                out.write(method)\n",
    "            # with open(f'method/method_{i}.py', 'w') as out:\n",
    "            #     out.write(method)\n",
    "\n",
    "            with open('test.py', 'w') as out:\n",
    "                out.write(data['test'])\n",
    "            \n",
    "            filtered_dir = 'filtered'\n",
    "            os.makedirs(filtered_dir, exist_ok=True)\n",
    "\n",
    "            with open(f'{filtered_dir}/method.py', 'w') as out:\n",
    "                out.write(method)\n",
    "\n",
    "            generate_split_test_files('test.py', filtered_dir, method_name, method)\n",
    "\n",
    "            lowest_coverage = 100\n",
    "            testbase = ''\n",
    "            for j in range(len(os.listdir(filtered_dir))-2):\n",
    "                unit_test_path = f'{filtered_dir}/assertion_{j+1}.py'\n",
    "                with open(unit_test_path) as f:\n",
    "                    subprocess.run([\"coverage\", \"run\", unit_test_path], check=True)\n",
    "                    result = subprocess.run([\"coverage\", \"report\", \"-m\"], capture_output=True, text=True, check=True)\n",
    "                    match = re.search(r'method.py.* (\\d+%)', result.stdout)\n",
    "                    if match:\n",
    "                        coverage_percentage = match.group(1)\n",
    "                        coverage_percentage = float(coverage_percentage[:-1])\n",
    "                    else:\n",
    "                        coverage_percentage = 0\n",
    "                    \n",
    "                    if coverage_percentage <= lowest_coverage:\n",
    "                        lowest_coverage = coverage_percentage\n",
    "                        testbase = f.read()\n",
    "            \n",
    "            data['coverage_testbase'] = lowest_coverage\n",
    "            data['testbase'] = testbase\n",
    "            # with open(f'testbase/testbase_{i}.py', 'w') as out:\n",
    "            #     test_ = testbase.replace('from method import', f'from method.method_{i} import')\n",
    "            #     out.write(test_)\n",
    "\n",
    "            shutil.rmtree(filtered_dir)\n",
    "\n",
    "            with open(f'{save_parsing_testbase_path}/{i}.json', 'w') as out:\n",
    "                json.dump(data, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from method import poly\n",
      "from method import find_zero\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "METADATA = {}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    import math\n",
      "    import random\n",
      "    rng = random.Random(42)\n",
      "    import copy\n",
      "    for _ in range(100):\n",
      "        ncoeff = 2 * rng.randint(1, 4)\n",
      "        coeffs = []\n",
      "        for _ in range(ncoeff):\n",
      "            coeff = rng.randint(-10, 10)\n",
      "            if coeff == 0:\n",
      "                coeff = 1\n",
      "            coeffs.append(coeff)\n",
      "        solution = candidate(copy.deepcopy(coeffs))\n",
      "        assert math.fabs(poly(coeffs, solution)) < 1e-4\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    check(find_zero)\n",
      "from method import encode_cyclic\n",
      "from method import decode_cyclic\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "METADATA = {}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    from random import randint, choice\n",
      "    import string\n",
      "\n",
      "    letters = string.ascii_lowercase\n",
      "    for _ in range(100):\n",
      "        str = ''.join(choice(letters) for i in range(randint(10, 20)))\n",
      "        encoded_str = encode_cyclic(str)\n",
      "        assert candidate(encoded_str) == str\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    check(decode_cyclic)\n",
      "from method import encode_shift\n",
      "from method import decode_shift\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "METADATA = {}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    from random import randint, choice\n",
      "    import copy\n",
      "    import string\n",
      "\n",
      "    letters = string.ascii_lowercase\n",
      "    for _ in range(100):\n",
      "        str = ''.join(choice(letters) for i in range(randint(10, 20)))\n",
      "        encoded_str = encode_shift(str)\n",
      "        assert candidate(copy.deepcopy(encoded_str)) == str\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    check(decode_shift)\n",
      "from method import change_base\n",
      "\n",
      "\n",
      "\n",
      "METADATA = {}\n",
      "\n",
      "def check(candidate):\n",
      "    assert (candidate(8, 3) == '22')\n",
      "    assert (candidate(9, 3) == '100')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    check(change_base)\n",
      "from method import add\n",
      "\n",
      "\n",
      "\n",
      "METADATA = {}\n",
      "\n",
      "def check(candidate):\n",
      "    import random\n",
      "    assert (candidate(0, 1) == 1)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    check(add)\n",
      "from method import double_the_difference\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert (candidate([]) == 0), 'This prints if this assert fails 1 (good for debugging!)'\n",
      "    assert (candidate([5, 4]) == 25), 'This prints if this assert fails 2 (good for debugging!)'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    check(double_the_difference)\n"
     ]
    }
   ],
   "source": [
    "for i in [32, 38, 50, 44, 53, 151]: \n",
    "    with open(f'{save_parsing_testbase_path}/{i}.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        method = data['prompt'] + data['canonical_solution']\n",
    "        method_name = data['entry_point']\n",
    "        with open('method.py', 'w') as out:\n",
    "            out.write(method)\n",
    "        # with open(f'method/method_{i}.py', 'w') as out:\n",
    "        #     out.write(method)\n",
    "\n",
    "        test = \"\"\n",
    "        tree = ast.parse(method)\n",
    "        functions = [node.name for node in tree.body if isinstance(node, ast.FunctionDef)]\n",
    "        for j in range(len(functions)):\n",
    "            test += f\"from method import {functions[j]}\\n\"\n",
    "        test += \"\\n\\n\"\n",
    "        test += data['testbase']\n",
    "        test += f\"\\nif __name__ == '__main__':\\n    check({method_name})\"\n",
    "        print(test)\n",
    "        with open('testbase.py', 'w') as out:\n",
    "            out.write(test)\n",
    "        # with open(f'testbase/testbase_{i}.py', 'w') as out:\n",
    "        #     test_ = test.replace('from method import', f'from method.method_{i} import')\n",
    "        #     out.write(test_)\n",
    "\n",
    "    subprocess.run([\"coverage\", \"run\", \"testbase.py\"], check=True)\n",
    "    result = subprocess.run([\"coverage\", \"report\", \"-m\"], capture_output=True, text=True, check=True)\n",
    "    match = re.search(r'method.py.* (\\d+%)', result.stdout)\n",
    "    if match:\n",
    "        coverage_percentage = match.group(1)\n",
    "        coverage_percentage = float(coverage_percentage[:-1])\n",
    "        if coverage_percentage != 100:\n",
    "            print(i)\n",
    "            print(result.stdout)\n",
    "            print(f\"Coverage: {coverage_percentage}\")\n",
    "    else:\n",
    "        coverage_percentage = None\n",
    "        print(\"Could not find coverage percentage.\")\n",
    "    data['coverage_testbase'] = coverage_percentage\n",
    "    with open(f'{save_parsing_testbase_path}/{i}.json', 'w') as out:\n",
    "        json.dump(data, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.makedirs(\"HumanEvalMethodAndTest\", exist_ok=True)\n",
    "num_samples = len(os.listdir('HumanEvalCoverageTestBase'))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    with open(f'HumanEvalCoverageTestBase/{i}.json') as f:\n",
    "        data = json.load(f)\n",
    "        with open(f'HumanEvalMethodAndTest/_{i}.py', 'w') as out:\n",
    "            out.write(data['prompt'] + data['canonical_solution'] + '\\n\\n\\n' + data['test'] + '\\n\\n\\n' + data['testbase'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coverage_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
